# 面试题目整理

## 操作系统

### 进程和线程的存储空间（共享与非共享）

#### 相关知识点：

1. **进程**：计算机上每个执行的活动，运行一个可执行程序是一个进程，打开一个软件是一个进程，打开一个终端是一个进程等等。
2. **多进程**：为了充分利用计算机资源产生了多进程的执行方式。通俗来讲就是在同一时间做多个事情，从而可以充分利用计算机资源还可以提高程序的执行效率。在创建一个新的子进程后，子进程会会获得计算机分配的资源，并拷贝父进程的数据。
3. **子进程与父进程**：我们编写的一段程序运行时就是一个进程，当我们在这段代码中调用特定函数新创建一个进程时，新创建的进程就是当前进程的子进程，而对于新创建的进程来说，当前进程就是它的父进程，子进程通过拷贝等手段继承父进程在创建子进程之前的数据、属性等。
4. **多线程**：上面说到进程是计算机上执行的每个活动。在实际编程中，一个进程中会有很多任务需要做。从而引发思考：一个进程中的多个任务是否也可以并发的执行？因为有些任务之间是没有联系的，也就是说有些任务完全可以同时进行而没有依赖问题。答案就是线程。与多进程相同的是，多线程也是为了让计算机资源得到充分利用并且程序的执行效率也会得到提高，与多进程不同的是，多线程并没有拷贝这种需求，它实际上是把一个进程分成多个片段。
5. **重入性**：尽管多线程系统开销少，但是也难免有缺陷，那就是重入性问题，使用多线程编程需要保证被多线程多次执行的函数的可重入性，所谓重入性就是函数被多个线程多次调用皆能正常执行。见附录2，有几个保证函数可重入性的条件。
6. **线程安全**：使用多线程编程除了需要保证函数的可重入性还需要保证线程安全，另外，可重入的函数一定是 线程安全的，但是反之不成立。
7. **共享内存**：根据字面意思就很好理解，通过把不同的进程中的逻辑内存映射到同一块物理内存中，进而允许两个进程共享同一块逻辑内存空间。它是进程间通信的一种方式。共享内存本身并没有提供同步机制。

--------

#### 定义上理解

​    1、进程是什么？
​      是具有一定独立功能的程序、它是系统进行资源分配和调度的一个独立单位，重点在系统调度和单独的单位，也就是说进程是可以独立运行的一段程序。
​    2、线程又是什么？
​     线程进程的一个实体，是CPU调度和分派的基本单位，他是比进程更小的能独立运行的基本单位，线程自己基本上不拥有系统资源。在运行时，只是暂用一些计数器、寄存器和栈 。

#### 他们之间的关系

​    1、一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程（通常说的主线程）。
​    2、资源分配给进程，同一进程的所有线程共享该进程的所有资源。
​    3、线程在执行过程中，需要协作同步。不同进程的线程间要利用消息通信的办法实现同步。
​    4、处理机分给线程，即真正在处理机上运行的是线程。
​    5、线程是指进程内的一个执行单元，也是进程内的可调度实体。  

#### 从三个角度来剖析二者之间的区别
​    1、调度：线程作为调度和分配的基本单位，进程作为拥有资源的基本单位。
​    2、并发性：不仅进程之间可以并发执行，同一个进程的多个线程之间也可以并发执行。
​    3、拥有资源：进程是拥有资源的一个独立单位，线程不拥有系统资源，但可以访问隶属于进程的资源。

------

#### 例子

做个简单的比喻：进程=火车，线程=车厢

- 线程在进程下行进（单纯的车厢无法运行）
- 一个进程可以包含多个线程（一辆火车可以有多个车厢）
- 不同进程间数据很难共享（一辆火车上的乘客很难换到另外一辆火车，比如站点换乘）
- 同一进程下不同线程间数据很易共享（A车厢换到B车厢很容易）
- 进程要比线程消耗更多的计算机资源（采用多列火车相比多个车厢更耗资源）
- 进程间不会相互影响，一个线程挂掉将导致整个进程挂掉（一列火车不会影响到另外一列火车，但是如果一列火车上中间的一节车厢与前一节产生断裂，将影响后面的所有车厢）
- 进程可以拓展到多机，进程最适合多核（不同火车可以开在多个轨道上，同一火车的车厢不能在行进的不同的轨道上）
- 进程使用的内存地址可以上锁，即一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。（比如火车上的洗手间）－"互斥锁"
- 进程使用的内存地址可以限定使用量（比如火车上的餐厅，最多只允许多少人进入，如果满了需要在门口等，等有人出来了才能进去）－“信号量”

### 用户态和内核态 

#### 相关知识点：

1. **内核态**：cpu可以访问内存的所有数据，包括外围设备，例如硬盘，网卡，cpu也可以将自己从一个程序切换到另一个程序。
2. **用户态**：只能受限的访问内存，且不允许访问外围设备，占用cpu的能力被剥夺，cpu资源可以被其他程序获取。

3. **为什么要有用户态和内核态？**

   由于需要限制不同的程序之间的访问能力, 防止他们获取别的程序的内存数据, 或者获取外围设备的数据, 并发送到网络, CPU划分出两个权限等级 -- 用户态和内核态。

4. **用户态与内核态的切换**

   所有用户程序都是运行在用户态的, 但是有时候程序确实需要做一些内核态的事情, 例如从硬盘读取数据, 或者从键盘获取输入等. 而唯一可以做这些事情的就是操作系统, 所以此时程序就需要先操作系统请求以程序的名义来执行这些操作.

   这时需要一个这样的机制: **用户态程序切换到内核态, 但是不能控制在内核态中执行的指令**

   **这种机制叫*系统调用*, 在CPU中的实现称之为*陷阱指令(Trap Instruction)***

   他们的工作流程如下:

   1. **用户态程序将一些数据值放在寄存器中, 或者使用参数创建一个堆栈(stack frame), 以此表明需要操作系统提供的服务.**
   2. **用户态程序执行陷阱指令**
   3. **CPU切换到内核态, 并跳到位于内存指定位置的指令, 这些指令是操作系统的一部分, 他们具有内存保护, 不可被用户态程序访问**
   4. **这些指令称之为陷阱(trap)或者系统调用处理器(system call handler). 他们会读取程序放入内存的数据参数, 并执行程序请求的服务**
   5. **系统调用完成后, 操作系统会重置CPU为用户态并返回系统调用的结果**

   当一个任务（进程）执行系统调用而陷入内核代码中执行时，我们就称进程处于内核运行态（或简称为内核态）。此时处理器处于特权级最高的（0级）内核代码中执行。当进程处于内核态时，执行的内核代码会使用当前进程的内核栈。每个进程都有自己的内核栈。当进程在执行用户自己的代码时，则称其处于用户运行态（用户态）。即此时处理器在特权级最低的（3级）用户代码中运行。当正在执行用户程序而突然被中断程序中断时，此时用户程序也可以象征性地称为处于进程的内核态。因为中断处理程序将使用当前进程的内核栈。这与处于内核态的进程的状态有些类似。 

   内核态与用户态是操作系统的两种运行级别,跟intel cpu没有必然的联系, **intel cpu提供Ring0-Ring3三种级别的运行模式，Ring0级别最高，Ring3最低。Linux使用了Ring3级别运行用户态，Ring0作为 内核态，没有使用Ring1和Ring2。Ring3状态不能访问Ring0的地址空间，包括代码和数据。**Linux进程的4GB地址空间，3G-4G部 分大家是共享的，是内核态的地址空间，这里存放在整个内核的代码和所有的内核模块，以及内核所维护的数据。用户运行一个程序，该程序所创建的进程开始是运 行在用户态的，如果要执行文件操作，网络数据发送等操作，必须通过write，send等系统调用，这些系统调用会调用内核中的代码来完成操作，这时，必 须切换到Ring0，然后进入3GB-4GB中的内核地址空间去执行这些代码完成操作，完成后，切换回Ring3，回到用户态。这样，用户态的程序就不能 随意操作内核地址空间，具有一定的安全保护作用。
   至于说保护模式，是说通过内存页表操作等机制，保证进程间的地址空间不会互相冲突，一个进程的操作不会修改另一个进程的地址空间中的数据。

------

#### 用户态切换到内核态的3种方式

**a. 系统调用**

这是用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作，比如前例中fork()实际上就是执行了一个创建新进程的系统调用。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。

**b. 异常**

当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。

**c. 外围设备的中断**

当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。

### 并行和并发

1. **并行(parallel)**：指在同一时刻，有多条指令在多个处理器上同时执行。所以无论从微观还是从宏观来看，二者都是一起执行的。

![img](https://upload-images.jianshu.io/upload_images/7557373-72912ea8e89c4007.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/313/format/webp)

2. **并发(concurrency)**：指在同一时刻只能有一条指令执行，但多个进程指令被快速的轮换执行，使得在宏观上具有多个进程同时执行的效果，但在微观上并不是同时执行的，只是把时间分成若干段，使多个进程快速交替的执行。

![img](https://upload-images.jianshu.io/upload_images/7557373-da64ffd6d1effaac.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/295/format/webp)

3. **区别：**

   1. 并行是指两个或者多个事件在同一时刻发生；而并发是指两个或多个事件在同一时间间隔发生。

   2. 并行是在不同实体上的多个事件，并发是在同一实体上的多个事件。

   3. 并行是在多台处理器上同时处理多个任务。如 hadoop 分布式集群，并发是在一台处理器上“同时”处理多个任务。

   4. 并行在多处理器系统中存在，而并发可以在单处理器和多处理器系统中都存在，并发能够在单处理器系统中存在是因为并发是并行的假象，并行要求程序能够同时执行多个操作，而并发只是要求程序假装同时执行多个操作（每个小时间片执行一个操作，多个操作快速切换执行）。

      

### 页面置换算法

#### 概念：

在地址映射过程中，若在页面中发现所要访问的页面不在内存中，则产生缺页中断。当发生缺页中断时，如果操作系统内存中没有空闲页面，则操作系统必须在内存选择一个页面将其移出内存，以便为即将调入的页面让出空间。而用来选择淘汰哪一页的规则叫做页面置换算法。

#### 常见的置换算法：

1. 最佳置换算法（OPT）
2. 先进先出置换算法（FIFO）
3. 最近最久未使用（LRU）算法

#### 最佳置换算法（OPT）：

- **基本思想：**发生缺页时，有些页面在内存中，其中有一页将很快被访问（也包含紧接着的下一条指令的那页），而其他页面则可能要到10、100或者1000条指令后才会被访问，每个页面都可以用在该页面首次被访问前所要执行的指令数进行标记。

- **缺点：**它**无法实现**。当缺页发生时，操作系统无法知道各个页面下一次是在什么时候被访问。

- **用处：**可以用于对可实现算法的性能进行衡量比较。

#### 先进先出置换算法（FIFO）：

- **基本思想：**总是选择在主存中停留时间最长（即最老）的一页置换，即先进入内存的页，先退出内存。最早调入内存的页，其不再被使用的可能性比刚调入内存的可能性大。建立一个FIFO队列，收容所有在内存中的页。被置换页面总是在队列头上进行。当一个页面被放入内存时，就把它插在队尾上。
- **缺点：**
  - 只是在按**线性顺序访问地址空间**时才是理想的，否则效率不高。因为那些常被访问的页，往往在主存中也停留得最久，结果它们因变“老”而不得不被置换出去。
  - FIFO的另一个缺点是，它有一种异常现象，即**在增加存储块的情况下，反而使缺页中断率增加了**。当然，导致这种异常现象的页面走向实际上是很少见的。

#### 最近最久未使用（LRU）：

- **基本思想：**当需要置换一页时，选择在之前一段时间里最久没有使用过的页面予以置换。
- **实现方式：**
  - **计数器：**最简单的情况是**使每个页表项对应一个使用时间字段**，并给**CPU增加一个逻辑时钟或计数器**。每次**存储访问，该时钟都加1**。每当访问一个页面时，时钟寄存器的内容就被复制到相应页表项的使用时间字段中。这样我们就可以始终保留着每个页面最后访问的“时间”。在置换页面时，选择该时间值最小的页面。这样做，不仅要查页表，而且当页表改变时（因CPU调度）要维护这个页表中的时间，还要考虑到时钟值溢出的问题。
  - **栈：**用一个**栈保留页号。每当访问一个页面时，就把它从栈中取出放在栈顶上**。这样一来，栈顶总是放有目前使用最多的页，而栈底放着目前最少使用的页。由于要从栈的中间移走一项，所以要用具有头尾指针的双向链连起来。在最坏的情况下，移走一页并把它放在栈顶上需要改动6个指针。每次修改都要有开销，但需要置换哪个页面却可直接得到，用不着查找，因为尾指针指向栈底，其中有被置换页。
- **缺点：**必须有**大量硬件支持**，还**需要一定的软件开销**。所以实际实现的都是一种简单有效的LRU近似算法。
- **近似算法：**
  - **最近未使用算法（Not Recently Used，NUR）**：在存储分块表的每一表项中增加一个引用位，操作系统定期地将它们置为0。当某一页被访问时，由硬件将该位置1。过一段时间后，通过检查这些位可以确定哪些页使用过，哪些页自上次置0后还未使用过。就可把该位是0的页淘汰出去，因为在之前最近一段时间里它未被访问过。
  - **Clock置换算法（LRU算法的近似实现）：**
  - **最少使用（LFU）置换算法**：为在内存中的每个页面设置一个移位寄存器，用来记录该页面被访问的频率。选择在之前时期使用最少的页面作为淘汰页。每次访问某页时，便将该移位寄存器的最高位置1，再每隔一定时间(例如100 ns)右移一次。这样，在最近一段时间使用最少的页面将是∑Ri（偏移量）最小的页。
  - **工作集算法**：

### 线程安全相关问题

#### 什么是线程安全性？

- 当多个线程访问某个类,不管运行时环境采用何种调度方式或者这些线程如何交替执行,并且在主调代码中不需
  要任何额外的同步或协同,这个类都能表现出正确的行为,那么就称这个类为线程安全的。

- 什么是线程不安全？
  多线程并发访问时，得不到正确的结果。

#### 原子性操作

##### 什么是原子性操作

一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。

##### 例子

- A想要从自己的帐户中转1000块钱到B的帐户里。那个从A开始转帐，到转帐结束的这一个过程，称之为一个事务。在这个事务里，要做如下操作： 从A的帐户中减去1000块钱。如果A的帐户原来有3000块钱，现在就变成2000块钱了。

- 在B的帐户里加1000块钱。如果B的帐户如果原来有2000块钱，现在则变成3000块钱了。如果在A的帐
  户已经减去了1000块钱的时候，忽然发生了意外，比如停电什么的，导致转帐事务意外终止了，而此时B的帐
  户里 还没有增加1000块钱。那么，我们称这个操作失败了，要进行回滚。回滚就是回到事务开始之前的状
  态，也就是回到A的帐户还没减1000块的状态，B的帐户的原来的状态。此时A的帐户仍然有3000块，B的帐
  户仍然有 2000块。

 通俗点讲：操作要成功一起成功、要失败大家一起失败如何把非原子性操作变成原子性

#### 如何避免线程安全性问题

##### 线程安全性问题成因

- 多线程环境
- 多个线程操作同一共享资源
- 对该共享资源进行了非原子性操作

##### 如何避免

打破成因中三点任意一点

1. 多线程环境 ==> **将多线程改单线程（必要的代码，加锁访问）**

2. 多个线程操作同一共享资源 ==> **不共享资源（ThreadLocal、不共享、操作无状态化、不可变）**

3. 对该共享资源进行了非原子性操作 ==> **将非原子性操作改成原子性操作（加锁、使用JDK自带的原子性操作的类、JUC提供的相应的并发工具类）**

##### 线程安全问题的解决方法：

1. **同步代码块**

   - **注意：**

     1. 通过代码块的锁对象，可以是任意的对象
     2. 必须保证多个线程使用的锁对象是同一个
     3. 锁对象的作用是把同步代码快锁住，只允许一个线程在同步代码块执行

   - **原理：**
     使用了一个锁对象，叫**同步锁，对象锁，也叫同步监视器**，当开启多个线程的时候，多个线程就开始抢夺CPU的执行权。

     比如现在t0线程首先的到执行，就会开始执行run方法，遇到同步代码快，首先检查是否有锁对象，发现有，则获取该锁对象，执行同步代码块中的代码。之后当CPU切换线程时，比如t1得到执行，也开始执行run方法，但是遇到同步代码块检查是否有锁对象时发现没有锁对象，t1便被阻塞，等待t0执行完毕同步代码块，释放锁对象，t1才可以获取从而进入同步代码块执行。

     同步中的线程，没有执行完毕是不会释放锁的，这样便实现了线程对临界区的互斥访问，保证了共享数据安全。

   - **缺点：**频繁的获取释放锁对象，降低程序效率

2. **同步方法**

   - 使用步骤：
     1. 把访问了共享数据的代码抽取出来，放到一个方法中
     2. 在该方法上添加 synchronized 修饰符（java）

3. **锁机制**

#### 锁的基本概念

##### 乐观锁

乐观锁是一种乐观思想，即认为**读多写少**，**遇到并发写的可能性低**，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，采取**在写时先读出当前版本号，然后加锁操作**（比较跟上一次的版本号，如果一样则更新），如果失败则要重复读-比较-写的操作。

##### 悲观锁

悲观锁是就是悲观思想，即认为**写多**，遇到并发写的可能性高，每次去拿数据的时候都认为别人会修改，所以**每次在读写数据的时候都会上锁**，这样别人想读写这个数据就会阻塞到拿到锁。

**性能问题：**悲观锁 -> 线程需要不断的进行上下文切换 -> 需要操作系统的介入 -> 上下文切换时间比同步代码执行的时间都长

### 进程间通信

　　**管道（pipe）**：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用进程间的亲缘关系通常是指父子进程关系。

　　**命名管道（named pipe/FIFO）**：命名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。

　　**信号量（semophonre）**：信号量是一个计数器，可以用来控制多个进程队共享资源的访问。它常作为一个锁机制，防止某进程在访问共享资源时，其他进程也访问此资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。

　　**消息队列（message queue）**：消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。

　　**信号（sinal）**：信号是一种比较复杂的通信方式，用于通知接受进程某个事件已经发生。

　　**共享内存（shared memory）**：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的ipc通信方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往和其他通信方式如信号量，配合使用来实现进程间的同步和通信。

　　**套接字（socket）**：套接字也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同设备间的进程通信。

　　**全双工管道**：共享内存、信号量、消息队列、管道和命名管道只适用于本地进程间通信，套接字和全双工管道可用于远程通信，因此可用于网络编程。

### 线程间通信

　　**锁机制**：包括互斥锁、条件变量、读写锁

　　　　互斥锁：提供了以排他方式防止数据结构被并发修改的方法。

　　　　读写锁：允许多个线程同时共享数据，而对写操作是互斥的。

　　　　条件变量：可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。

　　**信号量机制（Semaphore）**：包括无名进程信号量和命名线程信号量

　　**信号机制（Signal）**：类似进程间的信号处理

----------

## 计算机网络

![img](https://image.fundebug.com/2019-03-21-01.png)

### UDP

UDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。在OSI模型中，在第四层——传输层，处于IP协议的上一层。UDP有不提供数据包分组、组装和不能对数据包进行排序的缺点，也就是说，当报文发送之后，是无法得知其是否安全完整到达的。

它有以下几个特点：

#### 1. 面向无连接

首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。

具体来说就是：

- 在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了
- 在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作

#### 2. 有单播，多播，广播的功能

UDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。

#### 3. UDP是面向报文的

发送方的UDP对应用程序交下来的报文，在添加首部后就向下交付IP层。UDP对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。因此，应用程序必须选择合适大小的报文

#### 4. 不可靠性

首先不可靠性体现在无连接上，通信都不需要建立连接，想发就发，这样的情况肯定不可靠。

并且收到什么数据就传递什么数据，并且也不会备份数据，发送数据也不会关心对方是否已经正确接收到数据了。

再者网络环境时好时坏，但是 UDP 因为没有拥塞控制，一直会以恒定的速度发送数据。即使网络条件不好，也不会对发送速率进行调整。这样实现的弊端就是在网络条件不好的情况下可能会导致丢包，但是优点也很明显，在某些实时性要求高的场景（比如电话会议）就需要使用 UDP 而不是 TCP。

![img](https://image.fundebug.com/2019-03-21-02.gif)

#### 5. 头部开销小，传输数据报文时是很高效的。

![img](https://image.fundebug.com/2019-03-21-03.png)

UDP 头部包含了以下几个数据：

- 两个十六位的端口号，分别为源端口（可选字段）和目标端口
- 整个数据报文的长度
- 整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误

因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的

-----------------------------

### TCP

当一台计算机想要与另一台计算机通讯时，两台计算机之间的通信需要畅通且可靠，这样才能保证正确收发数据。例如，当你想查看网页或查看电子邮件时，希望完整且按顺序查看网页，而不丢失任何内容。当你下载文件时，希望获得的是完整的文件，而不仅仅是文件的一部分，因为如果数据丢失或乱序，都不是你希望得到的结果，于是就用到了TCP。

TCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，由 IETF 的RFC 793定义。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构，你可以把它想象成排水管中的水流。

#### 1. TCP连接过程

如下图所示，可以看到建立一个TCP连接的过程为（三次握手的过程）:

![img](https://image.fundebug.com/2019-03-21-04.png)

**第一次握手**

客户端向服务端发送连接请求报文段。该报文段中包含自身的数据通讯初始序号。请求发送后，客户端便进入 SYN-SENT 状态。

**第二次握手**

服务端收到连接请求报文段后，如果同意连接，则会发送一个应答，该应答中也会包含自身的数据通讯初始序号，发送完成后便进入 SYN-RECEIVED 状态。

**第三次握手**

当客户端收到连接同意的应答后，还要向服务端发送一个确认报文。客户端发完这个报文段后便进入 ESTABLISHED 状态，服务端收到这个应答后也进入 ESTABLISHED 状态，此时连接建立成功。

这里可能大家会有个疑惑：为什么 TCP 建立连接需要三次握手，而不是两次？这是因为这是为了防止出现失效的连接请求报文段被服务端接收的情况，从而产生错误。

![img](https://image.fundebug.com/2019-03-21-05.gif)

#### 2. TCP断开链接

![img](https://image.fundebug.com/2019-03-21-06.png)

TCP 是全双工的，在断开连接时两端都需要发送 FIN 和 ACK。

**第一次握手**

若客户端 A 认为数据发送完成，则它需要向服务端 B 发送连接释放请求。

**第二次握手**

B 收到连接释放请求后，会告诉应用层要释放 TCP 链接。然后会发送 ACK 包，并进入 CLOSE_WAIT 状态，此时表明 A 到 B 的连接已经释放，不再接收 A 发的数据了。但是因为 TCP 连接是双向的，所以 B 仍旧可以发送数据给 A。

**第三次握手**

B 如果此时还有没发完的数据会继续发送，完毕后会向 A 发送连接释放请求，然后 B 便进入 LAST-ACK 状态。

**第四次握手**

A 收到释放请求后，向 B 发送确认应答，此时 A 进入 TIME-WAIT 状态。该状态会持续 2MSL（最大段生存期，指报文段在网络中生存的时间，超时会被抛弃） 时间，若该时间段内没有 B 的重发请求的话，就进入 CLOSED 状态。当 B 收到确认应答后，也便进入 CLOSED 状态。

#### 3. TCP协议的特点

- 面向连接

  面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。

- 仅支持单播传输

每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。

- 面向字节流

TCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。

- 可靠传输

  对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。

- 提供拥塞控制

当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞

- TCP提供全双工通信

TCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）

### TCP和UDP的比较

#### 1. 对比

|              | UDP                                        | TCP                                    |
| :----------- | :----------------------------------------- | :------------------------------------- |
| 是否连接     | 无连接                                     | 面向连接                               |
| 是否可靠     | 不可靠传输，不使用流量控制和拥塞控制       | 可靠传输，使用流量控制和拥塞控制       |
| 连接对象个数 | 支持一对一，一对多，多对一和多对多交互通信 | 只能是一对一通信                       |
| 传输方式     | 面向报文                                   | 面向字节流                             |
| 首部开销     | 首部开销小，仅8字节                        | 首部最小20字节，最大60字节             |
| 适用场景     | 适用于实时应用（IP电话、视频会议、直播等） | 适用于要求可靠传输的应用，例如文件传输 |

#### 2. 总结

- TCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。
- 虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为
- 对数据准确性要求高，速度可以相对较慢的，可以选用TCP

------------

### HTTP1.0/1.1/2.0区别

- HTTP1.1默认使用长连接，可有效减少TCP的三次握手开销。
- HTTP 1.1支持只发送header信息(不带任何body信息)，如果服务器认为客户端有权限请求服务器，则返回100，否则返回401。客户端如果接受到100，才开始把请求body发送到服务器。这样当服务器返回401的时候，客户端就可以不用发送请求body了，节约了带宽。另外HTTP还支持传送内容的一部分。这样当客户端已经有一部分的资源后，只需要跟服务器请求另外的部分资源即可。这是支持文件断点续传的基础。
- HTTP1.0是没有host域的，HTTP1.1才支持这个参数。
- HTTP2.0使用多路复用技术(Multiplexing),多路复用允许同时通过单一的 HTTP/2 连接发起多重的请求-响应消息。
  "HTTP1.1在同一时间对于同一个域名的请求数量有限制，超过限制就会阻塞请求"。多路复用底层采用"增加二进制分帧层"的方法，使得不改变原来的语义、首部字段的情况下提高传输性能，降低延迟。
  二进制分帧将所有传输信息分割为更小的帧，用二进制进行编码，多个请求都在同一个TCP连接上完成，可以承载任意数量的双向数据流。HTTP/2更有效的使用TCP连接，得到性能上的提升。
  ![img](https://images2018.cnblogs.com/blog/951506/201803/951506-20180330005255437-1566386281.jpg)

### HTTP/2 新特性

#### 1.二进制传输

**HTTP/2传输数据量的大幅减少,主要有两个原因:以二进制方式传输和Header 压缩**。我们先来介绍二进制传输,HTTP/2 采用二进制格式传输数据，而非HTTP/1.x 里纯文本形式的报文 ，二进制协议解析起来更高效。 **HTTP/2 将请求和响应数据分割为更小的帧，并且它们采用二进制编码**。

它把TCP协议的部分特性挪到了应用层，把原来的"Header+Body"的消息"打散"为数个小片的二进制"帧"(Frame),用"HEADERS"帧存放头数据、"DATA"帧存放实体数据。HTP/2数据分帧后"Header+Body"的报文结构就完全消失了，协议看到的只是一个个的"碎片"。

![img](https:////upload-images.jianshu.io/upload_images/3174701-e8b190a7d93fd40b?imageMogr2/auto-orient/strip|imageView2/2/w/578/format/webp)

image

HTTP/2 中，同域名下所有通信都在单个连接上完成，该连接可以承载任意数量的双向数据流。每个数据流都以消息的形式发送，而消息又由一个或多个帧组成。**多个帧之间可以乱序发送，根据帧首部的流标识可以重新组装**。

#### Header 压缩

HTTP/2并没有使用传统的压缩算法，而是开发了专门的"HPACK”算法，在客户端和服务器两端建立“字典”，用索引号表示重复的字符串，还采用哈夫曼编码来压缩整数和字符串，可以达到50%~90%的高压缩率。

具体来说:

- 在客户端和服务器端使用“首部表”来跟踪和存储之前发送的键-值对，对于相同的数据，不再通过每次请求和响应发送；
- 首部表在HTTP/2的连接存续期内始终存在，由客户端和服务器共同渐进地更新;
- 每个新的首部键-值对要么被追加到当前表的末尾，要么替换表中之前的值

例如下图中的两个请求， 请求一发送了所有的头部字段，第二个请求则只需要发送差异数据，这样可以减少冗余数据，降低开销

![img](https:////upload-images.jianshu.io/upload_images/3174701-9e5e47286a5c143b?imageMogr2/auto-orient/strip|imageView2/2/w/506/format/webp)

#### 多路复用

在 HTTP/2 中引入了多路复用的技术。多路复用很好的解决了浏览器限制同一个域名下的请求数量的问题，同时也接更容易实现全速传输，毕竟新开一个 TCP 连接都需要慢慢提升传输速度。

大家可以通过 [该链接](https://links.jianshu.com/go?to=https%3A%2F%2Fhttp2.akamai.com%2Fdemo) 直观感受下 HTTP/2 比 HTTP/1 到底快了多少。

![img](https:////upload-images.jianshu.io/upload_images/3174701-6bd3b169bba46268?imageMogr2/auto-orient/strip|imageView2/2/w/680/format/webp)

 在 HTTP/2 中，有了二进制分帧之后，HTTP /2 不再依赖 TCP 链接去实现多流并行了，在 HTTP/2中,

- 同域名下所有通信都在单个连接上完成。
- 单个连接可以承载任意数量的双向数据流。
- 数据流以消息的形式发送，而消息又由一个或多个帧组成，多个帧之间可以乱序发送，因为根据帧首部的流标识可以重新组装。

这一特性，使性能有了极大提升：

- 同个域名只需要占用一个 TCP 连接，使用一个连接并行发送多个请求和响应,这样整个页面资源的下载过程只需要一次慢启动，同时也避免了多个TCP连接竞争带宽所带来的问题。
- 并行交错地发送多个请求/响应，请求/响应之间互不影响。
- 在HTTP/2中，每个请求都可以带一个31bit的优先值，0表示最高优先级， 数值越大优先级越低。有了这个优先值，客户端和服务器就可以在处理不同的流时采取不同的策略，以最优的方式发送流、消息和帧。

![img](https:////upload-images.jianshu.io/upload_images/3174701-273ba18448856c00?imageMogr2/auto-orient/strip|imageView2/2/w/745/format/webp)

如上图所示，多路复用的技术可以只通过一个 TCP 连接就可以传输所有的请求数据。

#### Server Push

HTTP2还在一定程度上改变了传统的“请求-应答”工作模式，服务器不再是完全被动地响应请求，也可以新建“流”主动向客户端发送消息。比如，在浏览器刚请求HTML的时候就提前把可能会用到的JS、CSS文件发给客户端，减少等待的延迟，这被称为"服务器推送"（ Server Push，也叫 Cache push）

例如下图所示,服务端主动把JS和CSS文件推送给客户端，而不需要客户端解析HTML时再发送这些请求。

![img](https:////upload-images.jianshu.io/upload_images/3174701-1f2eabe0f7cc7d43?imageMogr2/auto-orient/strip|imageView2/2/w/683/format/webp)

另外需要补充的是,服务端可以主动推送，客户端也有权利选择是否接收。如果服务端推送的资源已经被浏览器缓存过，浏览器可以通过发送RST_STREAM帧来拒收。主动推送也遵守同源策略，换句话说，服务器不能随便将第三方资源推送给客户端，而必须是经过双方确认才行。

#### 提高安全性

出于兼容的考虑，HTTP/2延续了HTTP/1的“明文”特点，可以像以前一样使用明文传输数据，不强制使用加密通信，不过格式还是二进制，只是不需要解密。

但由于HTTPS已经是大势所趋，而且主流的浏览器Chrome、Firefox等都公开宣布只支持加密的HTTP/2，**所以“事实上”的HTTP/2是加密的**。也就是说，互联网上通常所能见到的HTTP/2都是使用"https”协议名，跑在TLS上面。HTTP/2协议定义了两个字符串标识符：“h2"表示加密的HTTP/2，“h2c”表示明文的HTTP/2。

![img](https:////upload-images.jianshu.io/upload_images/3174701-28ccd7be587e4176?imageMogr2/auto-orient/strip|imageView2/2/w/583/format/webp)

### HTTP/3 新特性

#### HTTP/2 的缺点

虽然 HTTP/2 解决了很多之前旧版本的问题，但是它还是存在一个巨大的问题，**主要是底层支撑的 TCP 协议造成的**。HTTP/2的缺点主要有以下几点：

- TCP 以及 TCP+TLS建立连接的延时

HTTP/2使用TCP协议来传输的，而如果使用HTTPS的话，还需要使用TLS协议进行安全传输，而使用TLS也需要一个握手过程，**这样就需要有两个握手延迟过程**：

①在建立TCP连接的时候，需要和服务器进行三次握手来确认连接成功，也就是说需要在消耗完1.5个RTT之后才能进行数据传输。

②进行TLS连接，TLS有两个版本——TLS1.2和TLS1.3，每个版本建立连接所花的时间不同，大致是需要1~2个RTT。

总之，在传输数据之前，我们需要花掉 3～4 个 RTT。

- TCP的队头阻塞并没有彻底解决

上文我们提到在HTTP/2中，多个请求是跑在一个TCP管道中的。但当出现了丢包时，HTTP/2 的表现反倒不如 HTTP/1 了。因为TCP为了保证可靠传输，有个特别的“丢包重传”机制，丢失的包必须要等待重新传输确认，HTTP/2出现丢包时，整个 TCP 都要开始等待重传，那么就会阻塞该TCP连接中的所有请求（如下图）。而对于 HTTP/1.1 来说，可以开启多个 TCP 连接，出现这种情况反到只会影响其中一个连接，剩余的 TCP 连接还可以正常传输数据。

![img](https:////upload-images.jianshu.io/upload_images/3174701-4fd15ab2645fd0a2?imageMogr2/auto-orient/strip|imageView2/2/w/659/format/webp)

读到这里，可能就会有人考虑为什么不直接去修改 TCP 协议？其实这已经是一件不可能完成的任务了。因为 TCP 存在的时间实在太长，已经充斥在各种设备中，并且这个协议是由操作系统实现的，更新起来不大现实。

#### HTTP/3简介

Google 在推SPDY的时候就已经意识到了这些问题，于是就另起炉灶搞了一个基于 UDP 协议的“QUIC”协议，让HTTP跑在QUIC上而不是TCP上。
 而这个“HTTP over QUIC”就是HTTP协议的下一个大版本，HTTP/3。它在HTTP/2的基础上又实现了质的飞跃，真正“完美”地解决了“队头阻塞”问题。

![img](https:////upload-images.jianshu.io/upload_images/3174701-dc381a6a6308c3c1?imageMogr2/auto-orient/strip|imageView2/2/w/579/format/webp)

QUIC 虽然基于 UDP，但是在原本的基础上新增了很多功能，接下来我们重点介绍几个QUIC新功能。不过HTTP/3目前还处于草案阶段，正式发布前可能会有变动，所以本文尽量不涉及那些不稳定的细节。

#### QUIC新功能

上面我们提到QUIC基于UDP，而UDP是“无连接”的，根本就不需要“握手”和“挥手”，所以就比TCP来得快。此外QUIC也实现了可靠传输，保证数据一定能够抵达目的地。它还引入了类似HTTP/2的“流”和“多路复用”，单个“流"是有序的，可能会因为丢包而阻塞，但其他“流”不会受到影响。具体来说QUIC协议有以下特点：

- 实现了类似TCP的流量控制、传输可靠性的功能。

虽然UDP不提供可靠性的传输，但QUIC在UDP的基础之上增加了一层来保证数据可靠性传输。它提供了数据包重传、拥塞控制以及其他一些TCP中存在的特性。

- 实现了快速握手功能。

由于QUIC是基于UDP的，所以QUIC可以实现使用0-RTT或者1-RTT来建立连接，这意味着QUIC可以用最快的速度来发送和接收数据，这样可以大大提升首次打开页面的速度。**0RTT 建连可以说是 QUIC 相比 HTTP2 最大的性能优势**。

- 集成了TLS加密功能。

目前QUIC使用的是TLS1.3，相较于早期版本TLS1.3有更多的优点，其中最重要的一点是减少了握手所花费的RTT个数。

- 多路复用，彻底解决TCP中队头阻塞的问题

和TCP不同，QUIC实现了在同一物理连接上可以有多个独立的逻辑数据流（如下图）。实现了数据流的单独传输，就解决了TCP中队头阻塞的问题。

![img](https:////upload-images.jianshu.io/upload_images/3174701-87e1c496bb7c7e47?imageMogr2/auto-orient/strip|imageView2/2/w/685/format/webp)

道可用于远程通信，因此可用于网络编程。